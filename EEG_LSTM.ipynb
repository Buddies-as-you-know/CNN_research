{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "EEG_LSTM.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPz+chkn0mMOdm+ZoDyu35e",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YuminosukeSato/CNN_research/blob/main/EEG_LSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DeABf0-MH0nd"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import math\n",
        "from sklearn.manifold import TSNE\n",
        "from matplotlib import pyplot\n",
        "import pickle\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import TensorDataset\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "#import tqdm\n",
        "from torch.autograd import Variable\n",
        "from sklearn.metrics import confusion_matrix\n",
        "def cal_acc(t,p):\n",
        "    p_arg = torch.argmax(p,dim=1)\n",
        "    return torch.sum(t == p_arg)\n",
        "class Net(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        \n",
        "        super(Net, self).__init__()\n",
        "        self.seq_len = 50              # 画像の Height を時系列のSequenceとしてLSTMに入力する\n",
        "        self.feature_size = 432         # 画像の Width を特徴量の次元としてLSTMに入力する\n",
        "        self.hidden_layer_size = 30   # 隠れ層のサイズ\n",
        "        self.lstm_layers = 1           # LSTMのレイヤー数　(LSTMを何層重ねるか)\n",
        "        \n",
        "        self.lstm = nn.LSTM(self.feature_size, \n",
        "                            self.hidden_layer_size, \n",
        "                            num_layers = self.lstm_layers)\n",
        "        \n",
        "        self.fc = nn.Linear(self.hidden_layer_size, 7)\n",
        "        \n",
        "    def init_hidden_cell(self, batch_size, device): # LSTMの隠れ層 hidden と記憶セル cell を初期化\n",
        "        hedden = torch.zeros(self.lstm_layers, batch_size, self.hidden_layer_size).to(device)\n",
        "        cell = torch.zeros(self.lstm_layers, batch_size, self.hidden_layer_size).to(device)     \n",
        "        return (hedden, cell)\n",
        "\n",
        "    def forward(self, x, device):\n",
        "        batch_size = x.shape[0]\n",
        "        \n",
        "        self.hidden_cell = self.init_hidden_cell(batch_size, device)\n",
        "        \n",
        "        x = x.view(batch_size, self.seq_len, self.feature_size)  # (Batch, Cannel, Height, Width) -> (Batch, Height, Width) = (Batch, Seqence, Feature)\n",
        "                                                                 # 画像の Height を時系列のSequenceに、Width を特徴量の次元としてLSTMに入力する\n",
        "        x = x.permute(1, 0, 2)                                   # (Batch, Seqence, Feature) -> (Seqence , Batch, Feature)\n",
        "        \n",
        "        lstm_out, (h_n, c_n) = self.lstm(x, self.hidden_cell)    # LSTMの入力データのShapeは(Seqence, Batch, Feature)\n",
        "                                                                 # (h_n) のShapeは (num_layers, batch, hidden_size)\n",
        "        x = h_n[-1,:,:]                                          # lstm_layersの最後のレイヤーを取り出す  (B, h)\n",
        "        x = self.fc(x)\n",
        "        \n",
        "        return x\n",
        "\n",
        "def train_model(net, train_data, criterion, optimizer, device, num_epochs):\n",
        "    \n",
        "    # epochのループ\n",
        "    for epoch in range(num_epochs):\n",
        "        print('Epoch {}/{}'.format(epoch+1, num_epochs))\n",
        "        print('-------------')\n",
        "\n",
        "        # epochごとの学習と検証のループ\n",
        "        net.train()  # モデルを訓練モードに\n",
        "        epoch_loss = 0.0  # epochの損失和\n",
        "        epoch_corrects = 0  # epochの正解数\n",
        "\n",
        "            # 未学習時の検証性能を確かめるため、epoch=0の訓練は省略\n",
        "        if (epoch == 0):\n",
        "            continue\n",
        "\n",
        "            # データローダーからミニバッチを取り出すループ\n",
        "        for i , (inputs, labels) in enumerate(train_data,0):\n",
        "                \n",
        "          inputs = inputs.to(device)\n",
        "          labels = labels.to(device)\n",
        "                # optimizerを初期化\n",
        "          optimizer.zero_grad()\n",
        "          loss = criterion(outputs, labels.long())\n",
        "          loss.backward()\n",
        "          epoch_loss += loss.item() * inputs.size(0)  \n",
        "                    # 正解数の合計を更新\n",
        "          epoch_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "            # epochごとのlossと正解率を表示\n",
        "          epoch_loss = epoch_loss / len(train_data))\n",
        "          epoch_acc = epoch_corrects.double() / len(train_data)\n",
        "          print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = np.loadtxt(\"C:\\\\Users\\\\owner\\\\Desktop\\\\EEGdata\\\\train_data.txt\",dtype='float')\n",
        "train_label = np.loadtxt(\"C:\\\\Users\\\\owner\\\\Desktop\\\\EEGdata\\\\train_label.txt\",dtype='int')\n",
        "    #test data make\n",
        "    \n",
        "test_data = np.loadtxt(\"C:\\\\Users\\\\owner\\\\Desktop\\\\EEGdata\\\\test_data.txt\",dtype='float')\n",
        "test_label = np.loadtxt(\"C:\\\\Users\\\\owner\\\\Desktop\\\\EEGdata\\\\test_label.txt\",dtype='int')\n",
        "    #Tensor change\n",
        "train_data=torch.Tensor(train_data)\n",
        "train_label=torch.Tensor(train_label)\n",
        "test_data=torch.Tensor(test_data)\n",
        "test_label=torch.Tensor(test_label)\n",
        "    # form change\n",
        "train_data = train_data.view(100*7*4,50,432)\n",
        "test_data = test_data.view(100*7,50,432)\n",
        "    # data to dat5aset\n",
        "train_dataset = TensorDataset(train_data, train_label)\n",
        "test_dataset = TensorDataset(test_data, test_label)\n",
        "train_loader = DataLoader(train_dataset, batch_size=7, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset,batch_size=7,shuffle=False)\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
        "train_model(net, train_data, criterion, optimizer, device, num_epochs=num_epochs)\n",
        "net.eval() #推論モード\n",
        "with torch.set_grad_enabled(False)\n",
        "for n,(data,label) in enumerate(test_dataset,0):\n",
        "        data = data.to(device)\n",
        "        label = label.to(device)\n",
        "        data = data.view(7,1,50,432)\n",
        "        #p1d = (1, 403,2,20)\n",
        "        #data = F.pad(data, p1d, \"constant\", 0)\n",
        "        output = net(data,device)\n",
        "        test_total_acc += cal_acc(label.long(),output)\n",
        "        pred = torch.argmax(output , dim =1)\n",
        "        pred_list += pred.detach().cpu().numpy().tolist()\n",
        "        true_list += label.detach().cpu().numpy().tolist()\n",
        "        #print(pred_list)\n",
        "        #print(true_list)\n",
        "#print(f\"test acc:{test_total_acc/len(test_dataset)*100}\")\n",
        "cm = confusion_matrix(true_list, pred_list)\n",
        "sns.heatmap(cm, annot=True, cmap='Blues')\n",
        "plt.savefig('sklearn_confusion_matrix_annot_blues.png')"
      ],
      "metadata": {
        "id": "axwXP51fIOWM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}